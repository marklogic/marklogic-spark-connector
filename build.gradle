plugins {
  id 'java-library'
  id 'net.saliman.properties' version '1.5.2'
  id 'com.github.johnrengelman.shadow' version '8.1.1'
  id "com.marklogic.ml-gradle" version "5.0.0"
  id 'maven-publish'
  id "jacoco"
  id "org.sonarqube" version "5.1.0.4882"
}

group 'com.marklogic'
version '2.4-SNAPSHOT'

java {
  // To support reading RDF files, Apache Jena is used - but that requires Java 11.
  sourceCompatibility = 11
  targetCompatibility = 11
}

repositories {
  mavenCentral()
  mavenLocal()
  maven {
    url "https://bed-artifactory.bedford.progress.com:443/artifactory/ml-maven-snapshots/"
  }
}

configurations {
  // Defines all the implementation dependencies, but in such a way that they are not included as dependencies in the
  // library's pom.xml file. This is due to the shadow jar being published instead of a jar only containing this
  // project's classes. The shadow jar is published due to the need to relocate several packages to avoid conflicts
  // with Spark.
  shadowDependencies

  // This approach allows for all of the dependencies to be available for compilation and for running tests.
  compileOnly.extendsFrom(shadowDependencies)
  testImplementation.extendsFrom(compileOnly)
}

dependencies {
  // This is compileOnly as any environment this is used in will provide the Spark dependencies itself.
  compileOnly ('org.apache.spark:spark-sql_2.12:' + sparkVersion) {
    // Excluded from our ETL tool for size reasons, so excluded here as well to ensure we don't need it.
    exclude module: "rocksdbjni"
  }

  shadowDependencies ("com.marklogic:marklogic-client-api:7.0.0") {
    // The Java Client uses Jackson 2.15.2; Scala 3.4.x does not yet support that and will throw the following error:
    // Scala module 2.14.2 requires Jackson Databind version >= 2.14.0 and < 2.15.0 - Found jackson-databind version 2.15.2
    // So the 4 Jackson modules are excluded to allow for Spark's to be used.
    exclude group: "com.fasterxml.jackson.core"
    exclude group: "com.fasterxml.jackson.dataformat"
  }

  // Required for converting JSON to XML. Using 2.14.2 to align with Spark 3.4.3.
  shadowDependencies "com.fasterxml.jackson.dataformat:jackson-dataformat-xml:2.14.2"

  // Need this so that an OkHttpClientConfigurator can be created.
  shadowDependencies 'com.squareup.okhttp3:okhttp:4.12.0'

  shadowDependencies ("org.apache.jena:jena-arq:4.10.0") {
    exclude group: "com.fasterxml.jackson.core"
    exclude group: "com.fasterxml.jackson.dataformat"
  }

  shadowDependencies "org.jdom:jdom2:2.0.6.1"

  testImplementation ('com.marklogic:ml-app-deployer:5.0.0') {
    exclude group: "com.fasterxml.jackson.core"
    exclude group: "com.fasterxml.jackson.dataformat"

    // Use the Java Client declared above.
    exclude module: "marklogic-client-api"
  }

  testImplementation ('com.marklogic:marklogic-junit5:1.4.0') {
    exclude group: "com.fasterxml.jackson.core"
    exclude group: "com.fasterxml.jackson.dataformat"

    // Use the Java Client declared above.
    exclude module: "marklogic-client-api"
  }

  testImplementation "ch.qos.logback:logback-classic:1.3.14"
  testImplementation "org.slf4j:jcl-over-slf4j:2.0.13"
  testImplementation "org.skyscreamer:jsonassert:1.5.1"
}

test {
  useJUnitPlatform()
  finalizedBy jacocoTestReport
  // Allows mlHost to override the value in gradle.properties, which the test plumbing will default to.
  environment "mlHost", mlHost
}

// See https://docs.gradle.org/current/userguide/jacoco_plugin.html .
jacocoTestReport {
  dependsOn test
  reports {
    xml.required = true
  }
}

sonar {
  properties {
    property "sonar.projectKey", "marklogic-spark"
    property "sonar.host.url", "http://localhost:9000"
  }
}

task reloadTestData(type: com.marklogic.gradle.task.MarkLogicTask) {
  description = "Convenience task for clearing the test database and reloading the test data; only intended for a connector developer to use."
  doLast {
    new com.marklogic.mgmt.resource.databases.DatabaseManager(getManageClient()).clearDatabase("spark-test-test-content")
  }
}
reloadTestData.finalizedBy mlLoadData

if (JavaVersion.current().isCompatibleWith(JavaVersion.VERSION_17)) {
  test {
    // See https://stackoverflow.com/questions/72724816/running-unit-tests-with-spark-3-3-0-on-java-17-fails-with-illegalaccesserror-cl
    // for an explanation of why these are needed when running the tests on Java 17.
    jvmArgs = [
      '--add-exports=java.base/sun.nio.ch=ALL-UNNAMED',
      '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED',
      '--add-opens=java.base/sun.security.action=ALL-UNNAMED'
    ]
  }
}

shadowJar {
  configurations = [project.configurations.shadowDependencies]

  // "all" is the default; no need for that in the connector filename. This also results in this becoming the library
  // artifact that is published as a dependency. That is desirable as it includes the relocated packages listed below,
  // which a dependent would otherwise have to manage themselves.
  archiveClassifier.set("")

  // Spark uses an older version of OkHttp; see
  // https://stackoverflow.com/questions/61147800/how-to-override-spark-jars-while-running-spark-submit-command-in-cluster-mode
  // for more information on why these are relocated.
  relocate "okhttp3", "com.marklogic.okhttp3"
  relocate "okio", "com.marklogic.okio"
}

task perfTest(type: JavaExec) {
  mainClass = "com.marklogic.spark.reader.PerformanceTester"
  classpath = sourceSets.test.runtimeClasspath
  args mlHost
}

task sourcesJar(type: Jar, dependsOn: classes) {
  archiveClassifier = "sources"
  from sourceSets.main.allSource
}

task javadocJar(type: Jar, dependsOn: javadoc) {
  archiveClassifier = "javadoc"
  from javadoc
}
javadoc.failOnError = false
// Ignores warnings on params that don't have descriptions, which is a little too noisy
javadoc.options.addStringOption('Xdoclint:none', '-quiet')

artifacts {
  archives javadocJar, sourcesJar
}

publishing {
  publications {
    mainJava(MavenPublication) {
      pom {
        name = "${group}:${project.name}"
        description = "Spark 3 connector for MarkLogic"
        packaging = "jar"
        url = "https://github.com/marklogic/${project.name}"
        licenses {
          license {
            name = "The Apache License, Version 2.0"
            url = "http://www.apache.org/licenses/LICENSE-2.0.txt"
          }
        }
        developers {
          developer {
            id = "marklogic"
            name = "MarkLogic Github Contributors"
            email = "general@developer.marklogic.com"
            organization = "MarkLogic"
            organizationUrl = "https://www.marklogic.com"
          }
        }
        scm {
          url = "git@github.com:marklogic/${project.name}.git"
          connection = "scm:git@github.com:marklogic/${project.name}.git"
          developerConnection = "scm:git@github.com:marklogic/${project.name}.git"
        }
      }
      from components.java
      artifact sourcesJar
      artifact javadocJar
    }
  }
  repositories {
    maven {
      if (project.hasProperty("mavenUser")) {
        credentials {
          username mavenUser
          password mavenPassword
        }
        url publishUrl
        allowInsecureProtocol = true
      } else {
        name = "central"
        url = mavenCentralUrl
        credentials {
          username mavenCentralUsername
          password mavenCentralPassword
        }
      }
    }
  }
}

task gettingStartedZip(type: Zip) {
  description = "Creates a zip of the getting-started project that is intended to be included as a downloadable file " +
    "on the GitHub release page."
  from "examples/getting-started"
  exclude "build", ".gradle", "gradle-*.properties", ".venv", "venv", "docker"
  into "marklogic-spark-getting-started-${version}"
  archiveFileName = "marklogic-spark-getting-started-${version}.zip"
  destinationDirectory = file("build")
}
