plugins {
  id 'java-library'
  id 'net.saliman.properties' version '1.5.2'
  id 'com.github.johnrengelman.shadow' version '8.1.1'
  id "com.marklogic.ml-gradle" version "4.6.0"
  id 'maven-publish'
  id 'signing'
  id "jacoco"
  id "org.sonarqube" version "4.4.1.3373"
}

group 'com.marklogic'
version '2.2-SNAPSHOT'

java {
  // To support reading RDF files, Apache Jena is used - but that requires Java 11. If we want to do a 2.2.0 release
  // without requiring Java 11, we'll remove the support for reading RDF files along with the Jena dependency.
  sourceCompatibility = 11
  targetCompatibility = 11
}

repositories {
  mavenCentral()
}

configurations {
  mlcp {
    attributes {
      attribute(TargetJvmEnvironment.TARGET_JVM_ENVIRONMENT_ATTRIBUTE, objects.named(TargetJvmEnvironment.class, TargetJvmEnvironment.STANDARD_JVM))
    }
  }
}

dependencies {
  compileOnly 'org.apache.spark:spark-sql_2.12:' + sparkVersion
  implementation ("com.marklogic:marklogic-client-api:6.5.0") {
    // The Java Client uses Jackson 2.15.2; Scala 3.4.x does not yet support that and will throw the following error:
    // Scala module 2.14.2 requires Jackson Databind version >= 2.14.0 and < 2.15.0 - Found jackson-databind version 2.15.2
    // So the 4 Jackson modules are excluded to allow for Spark's to be used.
    exclude module: 'jackson-core'
    exclude module: 'jackson-databind'
    exclude module: 'jackson-annotations'
    exclude module: 'jackson-dataformat-csv'
  }

  // Need this so that an OkHttpClientConfigurator can be created.
  implementation 'com.squareup.okhttp3:okhttp:4.12.0'

  // Makes it possible to use lambdas in Java 8 to implement Spark's Function1 and Function2 interfaces
  // See https://github.com/scala/scala-java8-compat for more information
  implementation("org.scala-lang.modules:scala-java8-compat_2.12:1.0.2") {
    // Prefer the Scala libraries used within the user's Spark runtime.
    exclude module: "scala-library"
  }

  implementation "org.apache.jena:jena-arq:4.10.0"
  implementation "org.jdom:jdom2:2.0.6.1"

  // Only needed for loading data into the test-app.
  mlcp 'com.marklogic:mlcp:11.1.0'

  testImplementation 'org.apache.spark:spark-sql_2.12:' + sparkVersion

  // The exclusions in these two modules ensure that we use the Jackson libraries from spark-sql when running the tests.
  testImplementation ('com.marklogic:ml-app-deployer:4.7.0') {
    exclude module: 'jackson-core'
    exclude module: 'jackson-databind'
    exclude module: 'jackson-annotations'
    exclude module: 'jackson-dataformat-csv'
  }
  testImplementation ('com.marklogic:marklogic-junit5:1.4.0') {
    exclude module: 'jackson-core'
    exclude module: 'jackson-databind'
    exclude module: 'jackson-annotations'
    exclude module: 'jackson-dataformat-csv'
  }

  testImplementation "ch.qos.logback:logback-classic:1.3.14"
  testImplementation "org.slf4j:jcl-over-slf4j:1.7.36"
  testImplementation "org.skyscreamer:jsonassert:1.5.1"
}

test {
  useJUnitPlatform()
  finalizedBy jacocoTestReport
  // Allows mlHost to override the value in gradle.properties, which the test plumbing will default to.
  environment "mlHost", mlHost
}

jacocoTestReport {
  dependsOn test
  reports {
    xml.required = true
  }
}

sonar {
  properties {
    property "sonar.projectKey", "marklogic-spark"
    property "sonar.host.url", "http://localhost:9000"
  }
}

task reloadTestData(type: com.marklogic.gradle.task.MarkLogicTask) {
  description = "Convenience task for clearing the test database and reloading the test data; only intended for a connector developer to use."
  doLast {
    new com.marklogic.mgmt.resource.databases.DatabaseManager(getManageClient()).clearDatabase("spark-test-test-content")
  }
}
reloadTestData.finalizedBy mlLoadData

task deleteRdfCollection(type: com.marklogic.gradle.task.datamovement.DeleteCollectionsTask) {
  collections = ["http://marklogic.com/semantics#default-graph"]
  client = mlAppConfig.newTestDatabaseClient()
}

task loadTestTriples(type: com.marklogic.gradle.task.MlcpTask) {
  classpath = configurations.mlcp
  command = "IMPORT"
  port = Integer.parseInt(mlTestRestPort)
  input_file_path = "src/test/resources/rdf/1k.n3"
  input_file_type = "rdf"
  output_permissions = "spark-user-role,read,admin,update"
  output_collections = "http://marklogic.com/semantics#default-graph,test-config"
}
loadTestTriples.dependsOn deleteRdfCollection
mlDeployApp.finalizedBy loadTestTriples
mlLoadData.finalizedBy loadTestTriples

if (JavaVersion.current().isCompatibleWith(JavaVersion.VERSION_17)) {
  test {
    // See https://stackoverflow.com/questions/72724816/running-unit-tests-with-spark-3-3-0-on-java-17-fails-with-illegalaccesserror-cl
    // for an explanation of why these are needed when running the tests on Java 17.
    jvmArgs = [
      '--add-exports=java.base/sun.nio.ch=ALL-UNNAMED',
      '--add-opens=java.base/sun.util.calendar=ALL-UNNAMED'
    ]
  }
}

shadowJar {
  // "all" is the default; no need for that in the connector filename.
  archiveClassifier.set("")

  // Spark uses an older version of OkHttp; see
  // https://stackoverflow.com/questions/61147800/how-to-override-spark-jars-while-running-spark-submit-command-in-cluster-mode
  // for more information on why these are relocated.
  relocate "okhttp3", "com.marklogic.okhttp3"
  relocate "okio", "com.marklogic.okio"
}

task perfTest(type: JavaExec) {
  mainClass = "com.marklogic.spark.reader.PerformanceTester"
  classpath = sourceSets.test.runtimeClasspath
  args mlHost
}

task dockerBuildCache(type: Exec) {
  description = "Creates an image named 'marklogic-spark-cache' containing a cache of the Gradle dependencies."
  commandLine 'docker', 'build', '--no-cache', '-t', 'marklogic-spark-cache', '.'
}

task dockerTest(type: Exec) {
  description = "Run all of the tests within a Docker environment."
  commandLine 'docker', 'run',
    // Allows for communicating with the MarkLogic cluster that is setup via docker-compose.yaml.
    '--network=marklogic_spark_external_net',
    // Map the project directory into the Docker container.
    '-v', getProjectDir().getAbsolutePath() + ':/root/project',
    // Working directory for the Gradle tasks below.
    '-w', '/root/project',
    // Remove the container after it finishes running.
    '--rm',
    // Use the output of dockerBuildCache to avoid downloading all the Gradle dependencies.
    'marklogic-spark-cache:latest',
    'gradle', '-i', '-PmlHost=bootstrap_3n.local', 'test'
}

task dockerPerfTest(type: Exec) {
  description = "Run PerformanceTester a Docker environment."
  commandLine 'docker', 'run',
    '--network=marklogic_spark_external_net',
    '-v', getProjectDir().getAbsolutePath() + ':/root/project',
    '-w', '/root/project',
    '--rm',
    'marklogic-spark-cache:latest',
    'gradle', '-i', '-PmlHost=bootstrap_3n.local', 'perfTest'
}

task sourcesJar(type: Jar, dependsOn: classes) {
  archiveClassifier = "sources"
  from sourceSets.main.allSource
}

task javadocJar(type: Jar, dependsOn: javadoc) {
  archiveClassifier = "javadoc"
  from javadoc
}
javadoc.failOnError = false
// Ignores warnings on params that don't have descriptions, which is a little too noisy
javadoc.options.addStringOption('Xdoclint:none', '-quiet')

artifacts {
  archives javadocJar, sourcesJar
}
signing {
  sign configurations.archives
}

publishing {
  publications {
    mainJava(MavenPublication) {
      pom {
        name = "${group}:${project.name}"
        description = "Spark 3 connector for MarkLogic"
        packaging = "jar"
        url = "https://github.com/marklogic/${project.name}"
        licenses {
          license {
            name = "The Apache License, Version 2.0"
            url = "http://www.apache.org/licenses/LICENSE-2.0.txt"
          }
        }
        developers {
          developer {
            id = "marklogic"
            name = "MarkLogic Github Contributors"
            email = "general@developer.marklogic.com"
            organization = "MarkLogic"
            organizationUrl = "https://www.marklogic.com"
          }
        }
        scm {
          url = "git@github.com:marklogic/${project.name}.git"
          connection = "scm:git@github.com:marklogic/${project.name}.git"
          developerConnection = "scm:git@github.com:marklogic/${project.name}.git"
        }
      }
      from components.java
      artifact sourcesJar
      artifact javadocJar
    }
  }
  repositories {
    maven {
      if (project.hasProperty("mavenUser")) {
        credentials {
          username mavenUser
          password mavenPassword
        }
        url publishUrl
        allowInsecureProtocol = true
      } else {
        name = "central"
        url = mavenCentralUrl
        credentials {
          username mavenCentralUsername
          password mavenCentralPassword
        }
      }
    }
  }
}

task gettingStartedZip(type: Zip) {
  description = "Creates a zip of the getting-started project that is intended to be included as a downloadable file " +
    "on the GitHub release page."
  from "examples/getting-started"
  exclude "build", ".gradle", "gradle-*.properties", ".venv", "venv", "docker"
  into "marklogic-spark-getting-started-${version}"
  archiveFileName = "marklogic-spark-getting-started-${version}.zip"
  destinationDirectory = file("build")
}
